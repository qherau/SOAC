<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields</title>
	<!--<meta property="og:image" content="Path to my teaser.png"/>-->
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields" />
	<meta property="og:description" content="In rapidly-evolving domains such as autonomous driving, 
		the use of multiple sensors with different modalities is crucial to ensure high operational precision and stability. 
		To correctly exploit the provided information by each sensor in a single common frame, it is essential for these sensors to be accurately calibrated. 
		In this paper, we leverage the ability of Neural Radiance Fields (NeRF) to represent different sensors modalities in a common volumetric 
		representation to achieve robust and accurate spatio-temporal sensor calibration.
		By designing a partitioning approach based on the visible part of the scene for each sensor, 
		we formulate the calibration problem using only the overlapping areas. This strategy results in a more robust and accurate calibration that is less prone to failure. 
		We demonstrate that our approach works on outdoor urban scenes by validating it on multiple established driving datasets. 
		Results show that our method is able to get better accuracy and robustness compared to existing methods." />
	<meta name="google-site-verification" content="oerUbOFOxhAdGWX30cdj8RT69T7_30mlD8OZdDf02-g" />
	<meta name="description" content="In rapidly-evolving domains such as autonomous driving, 
		the use of multiple sensors with different modalities is crucial to ensure high operational precision and stability. 
		To correctly exploit the provided information by each sensor in a single common frame, it is essential for these sensors to be accurately calibrated. 
		In this paper, we leverage the ability of Neural Radiance Fields (NeRF) to represent different sensors modalities in a common volumetric 
		representation to achieve robust and accurate spatio-temporal sensor calibration.
		By designing a partitioning approach based on the visible part of the scene for each sensor, 
		we formulate the calibration problem using only the overlapping areas. This strategy results in a more robust and accurate calibration that is less prone to failure. 
		We demonstrate that our approach works on outdoor urban scenes by validating it on multiple established driving datasets. 
		Results show that our method is able to get better accuracy and robustness compared to existing methods.">
 	<meta name="keywords" content="calibration, multimodal, spatiotemporal, lidar, camera, nerf, neural radiance field, targetless, automatic, soac">	

</head>
	<script async src=""></script>
<body>
	<br>
	<center>
		<p>
			<span style="font-size:36px">SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields</span><br>
		</p>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/quentin-herau-38378b140/">Quentin Herau</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=S3zYmOYAAAAJ&hl">Nathan Piasco</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=0jLPiLYAAAAJ">Moussab Bennehar</a></span>
						</center>
					</td>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?hl=en&user=bRgp2lUAAAAJ">Luis Rold&atilde;o</a></span>
						</center>
					</td>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://www.linkedin.com/in/dzmitry-tsishkou-9b287724/">Dzmitry Tsishkou</a></span>
						</center>
					</td>
			
				</tr>
			</table>
			<table align=center width=1000px>

				<tr>

					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=PB2OanoAAAAJ&hl">Cyrille Migniot</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://scholar.google.com/citations?user=5dPw73sAAAAJ&hl">Pascal Vasseur</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:20px"><a href="https://sites.google.com/view/cedricdemonceaux/home">C&eacute;dric Demonceaux</a></span>
						</center>

			
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2311.15803'>[Paper]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p align="justify"><b>Method overview:</b> 
					SOAC is a novel multimodal spatiotemporal calibration method for cameras and LiDAR in the context of autonomous driving. 
					By alternating the training of multiple implicit scenes (section 3.2) and sensors co-registration from
					these representations (section 3.3), SOAC achieves precise self-supervised calibration from raw data acquired in unconstrained urban environments.</p>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p align="justify">In rapidly-evolving domains such as autonomous driving, 
				the use of multiple sensors with different modalities is crucial to ensure high operational precision and stability. 
				To correctly exploit the provided information by each sensor in a single common frame, it is essential for these sensors to be accurately calibrated. 
				In this paper, we leverage the ability of Neural Radiance Fields (NeRF) to represent different sensors modalities in a common volumetric 
				representation to achieve robust and accurate spatio-temporal sensor calibration.
				By designing a partitioning approach based on the visible part of the scene for each sensor, 
				we formulate the calibration problem using only the overlapping areas. This strategy results in a more robust and accurate calibration that is less prone to failure. 
				We demonstrate that our approach works on outdoor urban scenes by validating it on multiple established driving datasets. 
				Results show that our method is able to get better accuracy and robustness compared to existing methods.</p>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	
	
	<table align=center width=800px>
		<center><h1>Framework</h1></center>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Cite</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Q. Herau, N. Piasco, M. Bennehar, L. Rold&atilde;o, D. Tsishkou, C. Migniot, P. Vasseur, C. Demonceaux.<br>
				<b>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal calibration.</b><br>
				<!-- In Conference, 20XX.<br> -->
				(hosted on <a href="https://arxiv.org/abs/2311.15803">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

<br>
</body>
</html>

